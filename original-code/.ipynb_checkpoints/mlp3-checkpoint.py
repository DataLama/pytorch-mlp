# -*- coding: utf-8 -*-
"""MLP3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BkiycuaV2yeuJ9JzN_i_aF-_50blmX-A
"""

!python --version
!nvidia-smi
!apt-get update
!apt-get install g++ openjdk-8-jdk

!java --version

!pip install PyKomoran

!pip install -U scikit-learn

from PyKomoran import *
komoran=Komoran("EXP")

import numpy as np
import random
import torch
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, fbeta_score, f1_score

def set_seed():
  random.seed(777)
  np.random.seed(777)
  torch.manual_seed(777)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(777)
  set_seed()

#label list, mapping dict
label_list=['opening', 'request', 'wh-question', 'yn-question', 'inform', 'affirm', 'ack', 'expressive']
label_map = {label: i for i, label in enumerate(label_list)}

train_tfidf_list=list()
train_label_list=list()
test_tifdif_list=list()
test_label_list=list()

#train_data
with open('/content/drive/My Drive/Colab Notebooks/SpeechAct_tr.json') as json_file:
    tr_json_data=json.load(json_file)

tr_corpus=list()
for i in tr_json_data:
    if len(tr_json_data[i])==0:
        continue
    for j in range(len(tr_json_data[i])):
        tr_txt=tr_json_data[i][j][1]
        tr_corpus.append(tr_txt)

        tr_label=tr_json_data[i][j][2]
        train_label_list.append(label_map[tr_label])

tr_pos_list=list()
for sentence in tr_corpus:
    tr_pos_list.append('   '.join(komoran.get_morphes_by_tags(sentence, tag_list=['NNP', 'NNG', 'VV'])))

tfidfvect=TfidfVectorizer(token_pattern='(?u)\\b\\w+\\b')
tfidfvect.fit_transform(tr_pos_list)
train_tfidf_list = tfidfvect.transform(tr_pos_list).toarray().tolist()

#test_data
with open('/content/drive/My Drive/Colab Notebooks/SpeechAct_te.json') as json_file:
    te_json_data=json.load(json_file)

te_corpus=list()

for i in te_json_data:
    if len(te_json_data[i])==0:
        continue
    for j in range(len(te_json_data[i])):
        te_txt=te_json_data[i][j][1]
        te_corpus.append(te_txt)

        te_label = te_json_data[i][j][2]
        test_label_list.append(label_map[te_label])


te_pos_list=list()
for sentence in te_corpus:
    te_pos_list.append('   '.join(komoran.get_morphes_by_tags(sentence, tag_list=['NNP', 'NNG', 'VV'])))

test_tfidf_list = tfidfvect.transform(te_pos_list).toarray().tolist()

train_tfidf_tensor = torch.tensor(train_tfidf_list)
train_label_tensor = torch.tensor(train_label_list)
test_tfidf_tensor = torch.tensor(test_tfidf_list)
test_label_tensor = torch.tensor(test_label_list)

print(np.array(train_label_tensor).shape)
print(np.array(train_tfidf_tensor).shape)
print(np.array(test_label_tensor).shape)
print(np.array(test_tfidf_tensor).shape)

#device, model
class Perceptron(torch.nn.Module):
    def __init__(self, tfidf_size, num_label):
        super(Perceptron, self).__init__()
        self.linear1 = torch.nn.Linear(tfidf_size, 100)
        self.tanh1 = torch.nn.Tanh()
        self.linear2 = torch.nn.Linear(100, 8)
        self.tanh2 = torch.nn.Tanh()
        self.linear3 = torch.nn.Linear(8, num_label)

    def forward(self, tfidf_input):
        y_pred = self.linear1(tfidf_input)
        y_pred = self.tanh1(y_pred)
        y_pred = self.linear2(y_pred)
        y_pred = self.tanh2(y_pred)
        y_pred = self.linear3(y_pred)

        return y_pred

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Perceptron(tfidf_size=train_tfidf_tensor.shape[1], num_label=len(label_list))
model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

#데이터 묶기
Train_dataset = torch.utils.data.TensorDataset(train_tfidf_tensor, train_label_tensor)
Test_dataset = torch.utils.data.TensorDataset(test_tfidf_tensor, test_label_tensor)

#batch size 가져와서 학습
train_DataLoader = torch.utils.data.DataLoader(Train_dataset, shuffle=True, batch_size=4)
test_DataLoader = torch.utils.data.DataLoader(Test_dataset, shuffle=True, batch_size=1)

#Train
model.train(True)
model.zero_grad()
for epoch in range(500):
    epoch_loss = 0
    for batch in train_DataLoader:
        batch = tuple(t.to(device) for t in batch)
        y_pred = model(batch[0])

        loss = criterion(y_pred, batch[1])
        epoch_loss += loss.item()

        loss.backward()
        optimizer.step()
        model.zero_grad()

    if (epoch+1) % 10 ==0:
        print(epoch, epoch_loss)
model.train(False)

#Test
model.eval()
pred = None
label = None
for batch in test_DataLoader:
    batch = tuple(t.to(device) for t in batch)

    with torch.no_grad():
        y_pred = model(batch[0])

    if pred is None:
        pred = y_pred.detach().cpu().numpy()
        label = batch[1].detach().cpu().numpy()
    
    else:
        pred = np.append(pred, y_pred.detach().cpu().numpy(), axis=0)
        label = np.append(label, batch[1].detach().cpu().numpy(), axis=0)
y_pred = np.argmax(pred, axis=1)

matrix=confusion_matrix(test_label_tensor, y_pred)
accuracy = accuracy_score(test_label_tensor, y_pred)

macro_precision = precision_score(test_label_tensor, y_pred, average='macro')
micro_precision = precision_score(test_label_tensor, y_pred, average='micro')

macro_recall = recall_score(test_label_tensor, y_pred, average='macro')
micro_recall = recall_score(test_label_tensor, y_pred, average='micro')

macro_f1 = f1_score(test_label_tensor, y_pred, average='macro')
micro_f1 = f1_score(test_label_tensor, y_pred, average='micro')

print(accuracy)
print(macro_precision)
print(micro_precision)
print(macro_recall)
print(micro_recall)
print(macro_f1)
print(micro_f1)

fw = open('./2019711894_채나은_MLP.txt','w', encoding='UTF-8')
fw.write('Accuracy : '+str(accuracy))
fw.write('\n')
fw.write('Macro average precision : '+str(macro_precision))
fw.write('Micro average precision : '+str(micro_precision))
fw.write('\n')
fw.write('Macro average recall : '+str(macro_recall))
fw.write('Micro average recall : '+str(micro_recall))
fw.write('\n')
fw.write('Macro average f1-score : '+str(macro_f1))
fw.write('Micro average f1-score : '+str(micro_f1))
fw.close()

