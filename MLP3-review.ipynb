{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "안녕하세요.\n",
    "\n",
    "유경누나 친구입니다.\n",
    "\n",
    "최대한 코드를 변형하지 않는 선에서 코멘트 드릴게요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, fbeta_score, f1_score\n",
    "from PyKomoran import *\n",
    "komoran=Komoran(\"EXP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    random.seed(777)\n",
    "    np.random.seed(777)\n",
    "    torch.manual_seed(777)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(777)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label list, mapping dict\n",
    "label_list=['opening', 'request', 'wh-question', 'yn-question', 'inform', 'affirm', 'ack', 'expressive']\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "train_tfidf_list=list()\n",
    "train_label_list=list()\n",
    "test_tifdif_list=list()\n",
    "test_label_list=list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train, test data\n",
    "- 이 부분은 기존 코드로 사용하실 경우 함수화 해주시면 좋을 것 같습니다.\n",
    "- 아래와 같이 itertools.chain과 zip을 활용하면, 데이터를 좀 더 직관적으로 빠르게 불러올 수 있습니다.\n",
    "    - 파이썬에서는 왠만해서는 explicit하게 반복문을 안쓰는 것이 좋습니다. \n",
    "    - 특히, 딥러닝 하시면, 파이썬 코드를 최대한 효율적으로 짜는게 좋긴합니다. 경우에 따라서 5분이면 돌아갈 것이, 2~3시간 걸릴 수가 있습니다.\n",
    "    - `itertools`는 파이썬 기본 라이브러리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json data\n",
    "train_path = 'data/SpeechAct_tr.json'\n",
    "test_path = 'data/SpeechAct_te.json'\n",
    "\n",
    "with open(train_path) as json_file:\n",
    "    tr_json_data=json.load(json_file)\n",
    "    \n",
    "with open(test_path) as json_file:\n",
    "    te_json_data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- komoran tokenizer에서 tag_list를 제외했습니다.\n",
    "    - 딥러닝할 때는 전체 품사를 다 넣는 것이 좋습니다. \n",
    "    - Information loss로 성능하락의 원인이 될 수 있습니다.\n",
    "- TfidfVectorizer의 tokenizer로 사용하시는 토크나이저를 넘겨줬습니다.\n",
    "- tfidf를 list로 굳이 변환하지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## date preprocess and tfidfvectorizer\n",
    "# train\n",
    "# dictionary item에 대한 zip\n",
    "_ ,tr_corpus = list(zip(*tr_json_data.items()))\n",
    "\n",
    "# chain후 zip을 통해 sentence list, label list 분리.\n",
    "_, tr_corpus, train_label_list = list(zip(*chain(*tr_corpus))) \n",
    "\n",
    "# label index화\n",
    "train_label_list = [label_map[l] for l in train_label_list]\n",
    "\n",
    "# tfidf 정의\n",
    "tfidfvect = TfidfVectorizer(tokenizer=komoran.get_morphes_by_tags)\n",
    "\n",
    "# fit, transform, fit_transform의 차이를 이해하세요.\n",
    "train_tfidf_list = tfidfvect.fit_transform(tr_corpus).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "_ ,te_corpus = list(zip(*te_json_data.items()))\n",
    "_, te_corpus, test_label_list = list(zip(*chain(*te_corpus)))\n",
    "test_label_list = [label_map[l] for l in test_label_list]\n",
    "test_tfidf_list = tfidfvect.transform(te_corpus).toarray().tolist() # transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "train_tfidf_tensor = torch.tensor(train_tfidf_list)\n",
    "train_label_tensor = torch.tensor(train_label_list)\n",
    "test_tfidf_tensor = torch.tensor(test_tfidf_list)\n",
    "test_label_tensor = torch.tensor(test_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5825, 1117])\n",
      "torch.Size([5825])\n",
      "torch.Size([6671, 1117])\n",
      "torch.Size([6671])\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf_tensor.shape)\n",
    "print(train_label_tensor.shape)\n",
    "print(test_tfidf_tensor.shape)\n",
    "print(test_label_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size는 되도록이면 크게\n",
    "- test set은 shuffle 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256\n",
    "vocab_size = train_tfidf_tensor.shape[1]\n",
    "\n",
    "#데이터 묶기\n",
    "Train_dataset = torch.utils.data.TensorDataset(train_tfidf_tensor, train_label_tensor)\n",
    "Test_dataset = torch.utils.data.TensorDataset(test_tfidf_tensor, test_label_tensor)\n",
    "\n",
    "#batch size 가져와서 학습\n",
    "train_DataLoader = torch.utils.data.DataLoader(Train_dataset, shuffle=True, batch_size=bs, num_workers=16)\n",
    "test_DataLoader = torch.utils.data.DataLoader(Test_dataset, batch_size=bs, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden layer의 activation function은 특별한 이유가 없으면  ReLU로 하세요.\n",
    "- 학습이 안된 이유는 softmax함수를 안써서 그런거 같네요.\n",
    "    - 기존 코드는 logit을 계산하지 않고 loss계산을 해서 loss가 엄청 크게 나온 듯 하네요/\n",
    "    - CrossEntropyLoss로 loss 계산하시면, softmax 써주셔야됩니다.\n",
    "- pytorch 익숙하지 않으시면, tutorial 봐보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, label):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(vocab_size, 512)\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(512, 128)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(128, label)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_pred = self.linear1(X)\n",
    "        y_pred = self.relu1(y_pred)\n",
    "        y_pred = self.linear2(y_pred)\n",
    "        y_pred = self.relu2(y_pred)\n",
    "        y_pred = self.linear3(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Perceptron(vocab_size = vocab_size, label=len(label_list))\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(yhat, y):\n",
    "    with torch.no_grad():\n",
    "        yhat = yhat.max(dim=1)[1]\n",
    "        acc = (yhat == y).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1334: UserWarning: This overload of add_ is deprecated:\n",
      "add_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "add_(Tensor other, Number alpha)\n",
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1550: UserWarning: This overload of addcmul_ is deprecated:\n",
      "addcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "addcmul_(Tensor tensor1, Tensor tensor2, Number value)\n",
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1480: UserWarning: This overload of addcdiv_ is deprecated:\n",
      "addcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "addcdiv_(Tensor tensor1, Tensor tensor2, Number value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch - Train loss: 37.99076998233795 / Train accuracy: 0.5246483579925869\n",
      "1 epoch - Train loss: 19.41859859228134 / Train accuracy: 0.7631645643192789\n",
      "2 epoch - Train loss: 9.738964259624481 / Train accuracy: 0.8765566919160925\n",
      "3 epoch - Train loss: 6.80925615131855 / Train accuracy: 0.9045173111169235\n",
      "4 epoch - Train loss: 5.541881054639816 / Train accuracy: 0.9211578109989995\n",
      "5 epoch - Train loss: 4.903770610690117 / Train accuracy: 0.9320273762163909\n",
      "6 epoch - Train loss: 4.425178080797195 / Train accuracy: 0.937118965646495\n",
      "7 epoch - Train loss: 4.080668821930885 / Train accuracy: 0.9397184382314268\n",
      "8 epoch - Train loss: 3.8294343277812004 / Train accuracy: 0.9446991500647172\n",
      "9 epoch - Train loss: 3.630621001124382 / Train accuracy: 0.9471358268157296\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    tr_loss = 0.\n",
    "    tr_acc = 0.\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_DataLoader):\n",
    "        \n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        \n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        accuracy = acc(y_pred, labels)\n",
    "        tr_acc += accuracy.item()\n",
    "        tr_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    print(f\"{epoch} epoch - Train loss: {tr_loss} / Train accuracy: {tr_acc / (step+1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
