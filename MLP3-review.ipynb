{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, fbeta_score, f1_score\n",
    "from PyKomoran import *\n",
    "from model.utils import set_seed\n",
    "komoran=Komoran(\"EXP\")\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.MLP import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label list, mapping dict\n",
    "label_list=['opening', 'request', 'wh-question', 'yn-question', 'inform', 'affirm', 'ack', 'expressive']\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "train_tfidf_list=list()\n",
    "train_label_list=list()\n",
    "test_tifdif_list=list()\n",
    "test_label_list=list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset and Dataloader for train and test\n",
    "- 이 부분은 기존 코드로 사용하실 경우 함수화 해주시면 좋을 것 같습니다.\n",
    "- 아래와 같이 itertools.chain과 zip을 활용하면, 데이터를 좀 더 직관적으로 빠르게 불러올 수 있습니다.\n",
    "    - 파이썬에서는 왠만해서는 explicit하게 반복문을 안쓰는 것이 좋습니다. \n",
    "    - 특히, 딥러닝 하시면, 파이썬 코드를 최대한 효율적으로 짜는게 좋긴합니다. 경우에 따라서 5분이면 돌아갈 것이, 2~3시간 걸릴 수가 있습니다.\n",
    "    - `itertools`는 파이썬 기본 라이브러리입니다.\n",
    "    \n",
    "> 학습에 사용할 데이터 형태에 따라서 이 부분은 정말 다양하게 만들 수 있습니다. 다양한 모델 코드 돌려보면서 Dataset과 Dataloader부분을 정의하는 다양한 방식을 습득하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json data\n",
    "train_path = 'data/SpeechAct_tr.json'\n",
    "test_path = 'data/SpeechAct_te.json'\n",
    "\n",
    "with open(train_path) as json_file:\n",
    "    tr_json_data=json.load(json_file)\n",
    "    \n",
    "with open(test_path) as json_file:\n",
    "    te_json_data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- komoran tokenizer에서 tag_list를 제외했습니다.\n",
    "    - 딥러닝할 때는 전체 품사를 다 넣는 것이 좋습니다. \n",
    "    - Information loss로 성능하락의 원인이 될 수 있습니다.\n",
    "- TfidfVectorizer의 tokenizer로 사용하시는 토크나이저를 넘겨줬습니다.\n",
    "- tfidf를 list로 굳이 변환하지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## date preprocess and tfidfvectorizer\n",
    "# train\n",
    "# dictionary item에 대한 zip\n",
    "_ ,tr_corpus = list(zip(*tr_json_data.items()))\n",
    "\n",
    "# chain후 zip을 통해 sentence list, label list 분리.\n",
    "_, tr_corpus, train_label_list = list(zip(*chain(*tr_corpus))) \n",
    "\n",
    "# label index화\n",
    "train_label_list = [label_map[l] for l in train_label_list]\n",
    "\n",
    "# tfidf 정의\n",
    "tfidfvect = TfidfVectorizer(tokenizer=komoran.get_morphes_by_tags)\n",
    "\n",
    "# fit, transform, fit_transform의 차이를 이해하세요.\n",
    "train_tfidf_list = tfidfvect.fit_transform(tr_corpus).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "_ ,te_corpus = list(zip(*te_json_data.items()))\n",
    "_, te_corpus, test_label_list = list(zip(*chain(*te_corpus)))\n",
    "test_label_list = [label_map[l] for l in test_label_list]\n",
    "test_tfidf_list = tfidfvect.transform(te_corpus).toarray().tolist() # transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "train_tfidf_tensor = torch.tensor(train_tfidf_list)\n",
    "train_label_tensor = torch.tensor(train_label_list)\n",
    "test_tfidf_tensor = torch.tensor(test_tfidf_list)\n",
    "test_label_tensor = torch.tensor(test_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5825, 1117])\n",
      "torch.Size([5825])\n",
      "torch.Size([6671, 1117])\n",
      "torch.Size([6671])\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf_tensor.shape)\n",
    "print(train_label_tensor.shape)\n",
    "print(test_tfidf_tensor.shape)\n",
    "print(test_label_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size는 되도록이면 크게\n",
    "- test set은 shuffle 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256\n",
    "vocab_size = train_tfidf_tensor.shape[1]\n",
    "\n",
    "#데이터 묶기\n",
    "Train_dataset = torch.utils.data.TensorDataset(train_tfidf_tensor, train_label_tensor)\n",
    "Test_dataset = torch.utils.data.TensorDataset(test_tfidf_tensor, test_label_tensor)\n",
    "\n",
    "#batch size 가져와서 학습\n",
    "train_DataLoader = torch.utils.data.DataLoader(Train_dataset, shuffle=True, batch_size=bs, num_workers=2)\n",
    "test_DataLoader = torch.utils.data.DataLoader(Test_dataset, batch_size=bs, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden layer의 activation function은 특별한 이유가 없으면 ReLU가 좋습니다.\n",
    "- 항상 evaluate 기능을 만들어 두세요.\n",
    "    - 그래야지 overfitting이 발생하지 않는 최적의 지점을 찾을 수 있습니다.\n",
    "- 주의)\n",
    "    - 여기서는 test set으로 evaluate를 진행했는데, 원래 정석은 train set에서 일정 부분을 쪼개서 dev(validation) set을 구성해야됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Perceptron(vocab_size = vocab_size, label=len(label_list))\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1334: UserWarning: This overload of add_ is deprecated:\n",
      "add_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "add_(Tensor other, Number alpha)\n",
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1550: UserWarning: This overload of addcmul_ is deprecated:\n",
      "addcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "addcmul_(Tensor tensor1, Tensor tensor2, Number value)\n",
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1480: UserWarning: This overload of addcdiv_ is deprecated:\n",
      "addcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "addcdiv_(Tensor tensor1, Tensor tensor2, Number value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch - tr_loss : 1.8723 tr_acc 0.3653 tr_f1 0.1272  / val_loss : 1.5118 val_acc 0.3869 val_f1 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - tr_loss : 1.2299 tr_acc 0.5538 tr_f1 0.2745  / val_loss : 0.8779 val_acc 0.8108 val_f1 0.5276\n",
      "2 epoch - tr_loss : 0.6509 tr_acc 0.8414 tr_f1 0.5488  / val_loss : 0.4779 val_acc 0.8579 val_f1 0.5605\n",
      "3 epoch - tr_loss : 0.4066 tr_acc 0.8668 tr_f1 0.5754  / val_loss : 0.3589 val_acc 0.8750 val_f1 0.6070\n",
      "4 epoch - tr_loss : 0.3221 tr_acc 0.8910 tr_f1 0.7107  / val_loss : 0.2941 val_acc 0.9059 val_f1 0.7776\n",
      "5 epoch - tr_loss : 0.2673 tr_acc 0.9114 tr_f1 0.7888  / val_loss : 0.2525 val_acc 0.9174 val_f1 0.7960\n",
      "6 epoch - tr_loss : 0.2357 tr_acc 0.9226 tr_f1 0.8293  / val_loss : 0.2297 val_acc 0.9231 val_f1 0.8146\n",
      "7 epoch - tr_loss : 0.2139 tr_acc 0.9310 tr_f1 0.8441  / val_loss : 0.2190 val_acc 0.9306 val_f1 0.8541\n",
      "8 epoch - tr_loss : 0.2021 tr_acc 0.9324 tr_f1 0.8668  / val_loss : 0.2101 val_acc 0.9282 val_f1 0.8610\n",
      "9 epoch - tr_loss : 0.1916 tr_acc 0.9351 tr_f1 0.8700  / val_loss : 0.1985 val_acc 0.9358 val_f1 0.8692\n",
      "10 epoch - tr_loss : 0.1782 tr_acc 0.9408 tr_f1 0.8777  / val_loss : 0.1907 val_acc 0.9382 val_f1 0.8722\n",
      "11 epoch - tr_loss : 0.1697 tr_acc 0.9416 tr_f1 0.8795  / val_loss : 0.1848 val_acc 0.9403 val_f1 0.8863\n",
      "12 epoch - tr_loss : 0.1624 tr_acc 0.9428 tr_f1 0.8782  / val_loss : 0.1859 val_acc 0.9397 val_f1 0.8830\n",
      "13 epoch - tr_loss : 0.1598 tr_acc 0.9427 tr_f1 0.8867  / val_loss : 0.1777 val_acc 0.9421 val_f1 0.8893\n",
      "14 epoch - tr_loss : 0.1525 tr_acc 0.9482 tr_f1 0.8931  / val_loss : 0.1756 val_acc 0.9442 val_f1 0.8932\n",
      "15 epoch - tr_loss : 0.1477 tr_acc 0.9492 tr_f1 0.8966  / val_loss : 0.1728 val_acc 0.9444 val_f1 0.8945\n",
      "16 epoch - tr_loss : 0.1427 tr_acc 0.9506 tr_f1 0.8950  / val_loss : 0.1676 val_acc 0.9462 val_f1 0.8935\n",
      "17 epoch - tr_loss : 0.1403 tr_acc 0.9500 tr_f1 0.8969  / val_loss : 0.1690 val_acc 0.9460 val_f1 0.8963\n",
      "18 epoch - tr_loss : 0.1389 tr_acc 0.9497 tr_f1 0.8983  / val_loss : 0.1658 val_acc 0.9471 val_f1 0.8932\n",
      "19 epoch - tr_loss : 0.1328 tr_acc 0.9526 tr_f1 0.8993  / val_loss : 0.1607 val_acc 0.9481 val_f1 0.8928\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "    tr_loss = 0.\n",
    "    tr_y_pred = np.array([])\n",
    "    tr_y_true = np.array([])\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_DataLoader):\n",
    "        \n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        \n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        tr_y_pred = np.concatenate((tr_y_pred, y_pred.argmax(axis=1).numpy()))\n",
    "        tr_y_true = np.concatenate((tr_y_true, labels.numpy()))\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # train summary\n",
    "    tr_loss /= (step + 1)\n",
    "    \n",
    "    # calculate validation\n",
    "    val_loss = 0.\n",
    "    val_y_pred = np.array([])\n",
    "    val_y_true = np.array([])\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(test_DataLoader):\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(inputs)\n",
    "            loss = criterion(y_pred, labels)\n",
    "            \n",
    "        val_loss += loss.item()\n",
    "        val_y_pred = np.concatenate((val_y_pred, y_pred.argmax(axis=1).numpy()))\n",
    "        val_y_true = np.concatenate((val_y_true, labels.numpy()))\n",
    "    \n",
    "    val_loss /= (step + 1)\n",
    "    \n",
    "    tr_acc = accuracy_score(tr_y_true, tr_y_pred)\n",
    "    val_acc = accuracy_score(val_y_true, val_y_pred)\n",
    "    tr_f1 = f1_score(tr_y_true, tr_y_pred, average='macro')\n",
    "    val_f1 = f1_score(val_y_true, val_y_pred, average='macro')\n",
    "    \n",
    "    print(f\"{epoch} epoch - tr_loss : {tr_loss:.4f} tr_acc {tr_acc:.4f} tr_f1 {tr_f1:.4f}  / val_loss : {val_loss:.4f} val_acc {val_acc:.4f} val_f1 {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개선 방향\n",
    "- tensorboard를 적용해서 learning curve를 시각화해보세요.\n",
    "- early stopping을 적용해서 최적의 하이퍼파라미터를 찾아보세요.\n",
    "- 가장 좋은 성능을 보이는 모델을 저장해보세요.\n",
    "- inference.py를 만들어보세요.\n",
    "- 본인만의 딥러닝 템플릿을 만들어보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
